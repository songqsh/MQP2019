{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Nick's Copy of linearfunction01.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nickwotton/MQP2019/blob/master/Nick's_Copy_of_linearfunction01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pu5uIyYP6Tnz",
        "colab_type": "text"
      },
      "source": [
        "# Attempt to Improve Solving a Linear Function using a Nueral Network\n",
        "Given code to use a neural network to fit a linear function, try to optimize the code to get a better fit, i.e. the data points completely overlap on the plot."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KOvg3FsM7r51",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4PCo06270zm",
        "colab_type": "text"
      },
      "source": [
        "## Define the Function\n",
        "Here we define our function $f(x)=ax+b$ with coefficient $a=1$ and intercept term $b=2$.\n",
        "Then we test the equation with a test value of 2."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wdyGtXXn7r56",
        "outputId": "de7da3bb-b77d-4a1e-dfeb-425fc1323774",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# target function\n",
        "a = 1.\n",
        "b = 2.\n",
        "f = lambda x: a*x+b\n",
        "\n",
        "#test\n",
        "f(2.)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7G7JVcog6Ref",
        "colab_type": "text"
      },
      "source": [
        "## Create Model\n",
        "Next, we create the neural network model. This is done first by setting the inner and outer dimensions with variables. Next we code the model and vary the internal dimensions to attempt to improve the model. At this level, this is essentially a simple linear algebra exercise:\n",
        "\n",
        "If we have input $x$, internal parameters $a,b$, and solution $f(x)$ then in the one-dimensional case we have:\n",
        "\n",
        "\\begin{equation}\n",
        "  \\left(\n",
        "    a_{1}x+b_{2}\n",
        "  \\right)\n",
        "  a_{2} + b_{2}\n",
        "  = f(x)\n",
        "\\end{equation}\n",
        "  \n",
        "However, we want to get a better estimate for the true equation. So we increase the interior dimension which corresponds to the number of neurons inside the network. For example, we raised the inner dimension to 3. In matrix form we have:\n",
        "\n",
        "\\begin{equation}\n",
        "\\left(\n",
        "  \\begin{bmatrix} x \\end{bmatrix} \n",
        "  \\begin{bmatrix} a_{1} & a_{2} & a_{3} \\end{bmatrix} \n",
        "  + \n",
        "  \\begin{bmatrix} b_{1} & b_{2} & b_{3} \\end{bmatrix}\n",
        "\\right)\n",
        " \\begin{bmatrix} a_{4} \\\\ a_{5} \\\\ a_{6} \\end{bmatrix}\n",
        " +\n",
        " \\begin{bmatrix} b_{4} \\\\ \\end{bmatrix}\n",
        " =\n",
        " \\begin{bmatrix} f(x) \\end{bmatrix}\n",
        "\\end{equation}\n",
        "\n",
        "What we discovered here is that ReLU was slowing down the process, so since our function is Linear, we can just remove it. \n",
        "\n",
        "Additionally, we discerned that the higher the inner dimension, that is, the more nodes in each layer, the smaller the error and the better the performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CiVUxudQ7r5_",
        "colab": {}
      },
      "source": [
        "#model\n",
        "#nn.Linear\n",
        "in_dim = 1\n",
        "out_dim = 1\n",
        "\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(in_dim, 30),\n",
        "#    nn.ReLU(),\n",
        "    nn.Linear(30, out_dim)\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkuS30xvzTgV",
        "colab_type": "text"
      },
      "source": [
        "Here we define the Loss function as the Mean Squared Error(MSE). \n",
        "\n",
        "Note that by doing so, we are essentially 'cheating' the system. In most applications, we would not know the function $f$ so we would be unable to find the MSE."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7TqjOaLs7r6C",
        "colab": {}
      },
      "source": [
        "#loss function \n",
        "criterion = nn.MSELoss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIblNHIrzt8f",
        "colab_type": "text"
      },
      "source": [
        "Next we choose a learning rate and a method for learning. The learning rate is the percent of the data that is accepted in each iteration. The Methods we tried were SGD and Adam. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "E4rnJCtA7r6G",
        "colab": {}
      },
      "source": [
        "#optimizer\n",
        "learning_rate = 0.001\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-FHbIND2A34",
        "colab_type": "text"
      },
      "source": [
        "## Train the Model\n",
        "\n",
        "First we create the training data. This is a batch of random points that we pass through our function $f$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SbutUBbo7r6K",
        "colab": {}
      },
      "source": [
        "#training data\n",
        "\n",
        "batch_size = 1000\n",
        "\n",
        "x_train = torch.randn(batch_size, 1)\n",
        "y_train = f(x_train)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9VZMG4e2ziF",
        "colab_type": "text"
      },
      "source": [
        "Once we have the training data, we pass this collection of inputs and solutions into the model. With each iteration we calculate the loss and attempt to optimize the model to further reduce the loss.\n",
        "\n",
        "In this code we print out the loss every 500 iterations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4hNIFisK7r6M",
        "outputId": "53d7038c-b62a-4b15-e29b-226b25699052",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "# Train the model\n",
        "\n",
        "num_epochs = 10000\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    # Forward pass\n",
        "    outputs = model(x_train)\n",
        "    loss = criterion(outputs, y_train)\n",
        "    \n",
        "    # Backward and optimize\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    if (epoch+1) % 500 == 0:\n",
        "        print ('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, \n",
        "                                                    num_epochs, loss.item()))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch [500/10000], Loss: 0.0000\n",
            "Epoch [1000/10000], Loss: 0.0000\n",
            "Epoch [1500/10000], Loss: 0.0000\n",
            "Epoch [2000/10000], Loss: 0.0000\n",
            "Epoch [2500/10000], Loss: 0.0000\n",
            "Epoch [3000/10000], Loss: 0.0000\n",
            "Epoch [3500/10000], Loss: 0.0000\n",
            "Epoch [4000/10000], Loss: 0.0000\n",
            "Epoch [4500/10000], Loss: 0.0000\n",
            "Epoch [5000/10000], Loss: 0.0000\n",
            "Epoch [5500/10000], Loss: 0.0000\n",
            "Epoch [6000/10000], Loss: 0.0000\n",
            "Epoch [6500/10000], Loss: 0.0000\n",
            "Epoch [7000/10000], Loss: 0.0000\n",
            "Epoch [7500/10000], Loss: 0.0000\n",
            "Epoch [8000/10000], Loss: 0.0000\n",
            "Epoch [8500/10000], Loss: 0.0000\n",
            "Epoch [9000/10000], Loss: 0.0000\n",
            "Epoch [9500/10000], Loss: 0.0000\n",
            "Epoch [10000/10000], Loss: 0.0000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7-wQ_nA3eRF",
        "colab_type": "text"
      },
      "source": [
        "## Testing the Model\n",
        "Now that we have a trained model with low Loss, we want to attempt to replicate the function. To do this we get another random sample of numbers. This sample is passed into both our fuction $f$ and the model.\n",
        "\n",
        "We then graph both sets of points on a scatter plot. Since the model is highly accurate now, the two sets of points completely overlap."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qZjRYQGe7r6P",
        "outputId": "1189e9aa-367d-4af6-ef14-ee405c4f7c9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "#test\n",
        "x_ = torch.randn(50,1)\n",
        "y_ = f(x_)\n",
        "plt.scatter(x_.detach().numpy(), y_.detach().numpy(), label='true')\n",
        "\n",
        "y_pred = model(x_)\n",
        "plt.scatter(x_.detach().numpy(), y_pred.detach().numpy(), label='pred')\n",
        "\n",
        "plt.legend()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f9123afa8d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD8CAYAAABjAo9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEOwAEDP/D5gUQUcpn1uzAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IT6OV7pa7r6R"
      },
      "source": [
        "__Todo__\n",
        "\n",
        "- Improve the above code.\n",
        "- train a model to $f(x) = x^2 + 1$\n",
        "- train a model to \n",
        "$$f(x) = \\begin{bmatrix} \n",
        "1 & 1 \\\\\n",
        "0 & 1 \n",
        "\\end{bmatrix} x + \n",
        "\\begin{bmatrix} \n",
        "1  \\\\\n",
        "0  \n",
        "\\end{bmatrix}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Xm-OqVS_7r6S",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}